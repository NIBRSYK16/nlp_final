# ä½¿ç”¨æœ¬åœ° Qwen2.5-Coder-1.5B æ¨¡å‹çš„ Gradio ç•Œé¢
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import gradio as gr
import os

# å…¨å±€å˜é‡å­˜å‚¨æ¨¡å‹å’Œåˆ†è¯å™¨
model = None
tokenizer = None
device = None

# è®¾ç½®æœ¬åœ°æ¨¡å‹è·¯å¾„ï¼ˆè¯·æ ¹æ®ä½ çš„å®é™…è·¯å¾„ä¿®æ”¹ï¼‰
DEFAULT_MODEL_PATH = "./Qwen2.5-Coder-1.5B"

def load_model(model_path=None):
    """åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨"""
    global model, tokenizer, device
    
    # ä½¿ç”¨é»˜è®¤è·¯å¾„æˆ–ç”¨æˆ·æä¾›çš„è·¯å¾„
    if model_path is None or model_path.strip() == "":
        model_path = DEFAULT_MODEL_PATH
    
    # æ£€æŸ¥æ¨¡å‹è·¯å¾„æ˜¯å¦å­˜åœ¨
    if not os.path.exists(model_path):
        return f"é”™è¯¯ï¼šæ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}\nè¯·æ£€æŸ¥è·¯å¾„æ˜¯å¦æ­£ç¡®ã€‚"
    
    try:
        print(f"æ­£åœ¨ä»æœ¬åœ°è·¯å¾„åŠ è½½æ¨¡å‹: {model_path}")
        print("æ­£åœ¨åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨...")
        
        # ä»æœ¬åœ°è·¯å¾„åŠ è½½åˆ†è¯å™¨
        tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)
        
        # ç¡®å®šè®¾å¤‡
        device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"ä½¿ç”¨è®¾å¤‡: {device}")
        
        # ä»æœ¬åœ°è·¯å¾„åŠ è½½æ¨¡å‹
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            local_files_only=True,  # åªä½¿ç”¨æœ¬åœ°æ–‡ä»¶ï¼Œä¸ä»ç½‘ç»œä¸‹è½½
            dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            low_cpu_mem_usage=True,
            trust_remote_code=True
        )
        model = model.to(device)
        model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        print("æ¨¡å‹åŠ è½½å®Œæˆï¼")
        return f"âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼\næ¨¡å‹è·¯å¾„: {model_path}\nä½¿ç”¨è®¾å¤‡: {device}"
        
    except Exception as e:
        return f"âŒ åŠ è½½æ¨¡å‹æ—¶å‡ºé”™ï¼š{str(e)}"

def generate_code(prompt, system_prompt, max_tokens, temperature, top_p):
    """ç”Ÿæˆä»£ç çš„å‡½æ•°"""
    if model is None or tokenizer is None:
        return "é”™è¯¯ï¼šæ¨¡å‹å°šæœªåŠ è½½ï¼Œè¯·å…ˆç‚¹å‡»'åŠ è½½æ¨¡å‹'æŒ‰é’®ã€‚"
    
    if not prompt or prompt.strip() == "":
        return "é”™è¯¯ï¼šè¯·è¾“å…¥ä»£ç ç”Ÿæˆæç¤ºã€‚"
    
    try:
        # å‡†å¤‡å¯¹è¯æ¶ˆæ¯
        messages = [
            {"role": "system", "content": system_prompt if system_prompt else "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¼–ç¨‹åŠ©æ‰‹ï¼Œæ“…é•¿ç¼–å†™å’Œè§£é‡Šä»£ç ã€‚"},
            {"role": "user", "content": prompt},
        ]
        
        # åº”ç”¨èŠå¤©æ¨¡æ¿
        text = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        
        # å°†æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥
        model_inputs = tokenizer([text], return_tensors="pt").to(device)
        
        # ç”Ÿæˆä»£ç 
        with torch.no_grad():
            generated_ids = model.generate(
                **model_inputs,
                max_new_tokens=int(max_tokens),
                temperature=float(temperature),
                top_p=float(top_p),
                do_sample=True
            )
        
        # æå–ç”Ÿæˆçš„æ–‡æœ¬ï¼ˆå»æ‰è¾“å…¥éƒ¨åˆ†ï¼‰
        generated_ids = [
            output_ids[len(input_ids):] 
            for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
        ]
        
        response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
        return response
        
    except Exception as e:
        return f"ç”Ÿæˆä»£ç æ—¶å‡ºé”™ï¼š{str(e)}"

# åˆ›å»º Gradio ç•Œé¢
with gr.Blocks(title="Qwen2.5-Coder æœ¬åœ°æ¨¡å‹ä»£ç ç”Ÿæˆå™¨", theme=gr.themes.Soft()) as demo:
    gr.Markdown("# ğŸ¤– Qwen2.5-Coder æœ¬åœ°æ¨¡å‹ä»£ç ç”Ÿæˆå™¨")
    gr.Markdown("ä½¿ç”¨æœ¬åœ°ä¸‹è½½çš„ Qwen2.5-Coder-1.5B æ¨¡å‹ç”Ÿæˆä»£ç ã€‚")
    
    with gr.Row():
        with gr.Column():
            gr.Markdown("### ğŸ“ æ¨¡å‹è®¾ç½®")
            model_path_input = gr.Textbox(
                label="æ¨¡å‹è·¯å¾„",
                value=DEFAULT_MODEL_PATH,
                placeholder="è¾“å…¥æœ¬åœ°æ¨¡å‹è·¯å¾„ï¼Œä¾‹å¦‚: ./Qwen2.5-Coder-1.5B",
                lines=1
            )
            load_btn = gr.Button("ğŸ”„ åŠ è½½æ¨¡å‹", variant="primary", size="lg")
            load_status = gr.Textbox(label="æ¨¡å‹çŠ¶æ€", interactive=False, lines=3)
            
            with gr.Accordion("âš™ï¸ é«˜çº§è®¾ç½®", open=False):
                system_prompt_input = gr.Textbox(
                    label="ç³»ç»Ÿæç¤ºè¯",
                    value="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¼–ç¨‹åŠ©æ‰‹ï¼Œæ“…é•¿ç¼–å†™å’Œè§£é‡Šä»£ç ã€‚",
                    lines=2,
                    placeholder="è¾“å…¥ç³»ç»Ÿæç¤ºè¯..."
                )
                max_tokens_input = gr.Slider(
                    label="æœ€å¤§ç”Ÿæˆtokenæ•°",
                    minimum=50,
                    maximum=2048,
                    value=512,
                    step=50
                )
                temperature_input = gr.Slider(
                    label="Temperature (åˆ›é€ æ€§)",
                    minimum=0.1,
                    maximum=2.0,
                    value=0.7,
                    step=0.1
                )
                top_p_input = gr.Slider(
                    label="Top-p (æ ¸é‡‡æ ·)",
                    minimum=0.1,
                    maximum=1.0,
                    value=0.9,
                    step=0.05
                )
        
        with gr.Column():
            gr.Markdown("### ğŸ’» ä»£ç ç”Ÿæˆ")
            prompt_input = gr.Textbox(
                label="ä»£ç ç”Ÿæˆæç¤º",
                placeholder="ä¾‹å¦‚ï¼šè¯·ç”¨Pythonç¼–å†™ä¸€ä¸ªå¿«é€Ÿæ’åºç®—æ³•ã€‚",
                lines=5
            )
            generate_btn = gr.Button("âœ¨ ç”Ÿæˆä»£ç ", variant="primary", size="lg")
            output = gr.Code(
                label="ç”Ÿæˆçš„ä»£ç ",
                language="python",
                lines=20
            )
    
    # ç»‘å®šäº‹ä»¶
    load_btn.click(
        fn=load_model,
        inputs=model_path_input,
        outputs=load_status
    )
    
    generate_btn.click(
        fn=generate_code,
        inputs=[prompt_input, system_prompt_input, max_tokens_input, temperature_input, top_p_input],
        outputs=output
    )
    
    # ç¤ºä¾‹æç¤ºè¯
    gr.Examples(
        examples=[
            ["è¯·ç”¨Pythonç¼–å†™ä¸€ä¸ªå¿«é€Ÿæ’åºç®—æ³•ã€‚"],
            ["ç”¨Pythonå®ç°ä¸€ä¸ªç®€å•çš„HTTPæœåŠ¡å™¨ã€‚"],
            ["å†™ä¸€ä¸ªå‡½æ•°æ¥è®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—çš„ç¬¬né¡¹ã€‚"],
            ["ç”¨Pythonå®ç°ä¸€ä¸ªç®€å•çš„è®¡ç®—å™¨ç±»ã€‚"],
        ],
        inputs=prompt_input
    )

if __name__ == "__main__":
    # å¯åŠ¨ Gradio ç•Œé¢
    demo.launch(share=False, server_name="0.0.0.0", server_port=7860)

