# ä½¿ç”¨æœ¬åœ° Qwen2.5-Coder-1.5B æ¨¡å‹çš„ä¼˜åŒ–ç‰ˆ Gradio ç•Œé¢
# ä½¿ç”¨é‡åŒ–ç­‰æŠ€æœ¯åŠ é€Ÿæ¨ç†
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
import torch
import gradio as gr
import os

# å…¨å±€å˜é‡å­˜å‚¨æ¨¡å‹å’Œåˆ†è¯å™¨
model = None
tokenizer = None
device = None

# è®¾ç½®æœ¬åœ°æ¨¡å‹è·¯å¾„ï¼ˆè¯·æ ¹æ®ä½ çš„å®é™…è·¯å¾„ä¿®æ”¹ï¼‰
DEFAULT_MODEL_PATH = "./Qwen2.5-Coder-1.5B"

def load_model(model_path=None, use_quantization=True, quantization_type="8bit"):
    """åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
    global model, tokenizer, device
    
    # ä½¿ç”¨é»˜è®¤è·¯å¾„æˆ–ç”¨æˆ·æä¾›çš„è·¯å¾„
    if model_path is None or model_path.strip() == "":
        model_path = DEFAULT_MODEL_PATH
    
    # æ£€æŸ¥æ¨¡å‹è·¯å¾„æ˜¯å¦å­˜åœ¨
    if not os.path.exists(model_path):
        return f"é”™è¯¯ï¼šæ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}\nè¯·æ£€æŸ¥è·¯å¾„æ˜¯å¦æ­£ç¡®ã€‚"
    
    try:
        print(f"æ­£åœ¨ä»æœ¬åœ°è·¯å¾„åŠ è½½æ¨¡å‹: {model_path}")
        print("æ­£åœ¨åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨...")
        
        # ä»æœ¬åœ°è·¯å¾„åŠ è½½åˆ†è¯å™¨
        tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)
        
        # ç¡®å®šè®¾å¤‡
        device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"ä½¿ç”¨è®¾å¤‡: {device}")
        
        # é…ç½®é‡åŒ–ï¼ˆå¦‚æœä½¿ç”¨GPUä¸”å¯ç”¨é‡åŒ–ï¼‰
        quantization_config = None
        if use_quantization and torch.cuda.is_available():
            if quantization_type == "8bit":
                quantization_config = BitsAndBytesConfig(
                    load_in_8bit=True,
                    llm_int8_threshold=6.0,
                )
                print("ä½¿ç”¨ 8-bit é‡åŒ–åŠ è½½æ¨¡å‹...")
            elif quantization_type == "4bit":
                quantization_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_compute_dtype=torch.float16,
                    bnb_4bit_use_double_quant=True,
                    bnb_4bit_quant_type="nf4"
                )
                print("ä½¿ç”¨ 4-bit é‡åŒ–åŠ è½½æ¨¡å‹...")
        
        # ä»æœ¬åœ°è·¯å¾„åŠ è½½æ¨¡å‹
        load_kwargs = {
            "local_files_only": True,
            "trust_remote_code": True,
            "low_cpu_mem_usage": True,
        }
        
        if quantization_config:
            load_kwargs["quantization_config"] = quantization_config
        else:
            # å¦‚æœæ²¡æœ‰é‡åŒ–ï¼Œä½¿ç”¨åŠç²¾åº¦ï¼ˆGPUï¼‰æˆ–å…¨ç²¾åº¦ï¼ˆCPUï¼‰
            load_kwargs["dtype"] = torch.float16 if torch.cuda.is_available() else torch.float32
        
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            **load_kwargs
        )
        
        # å¦‚æœä½¿ç”¨é‡åŒ–ï¼Œæ¨¡å‹å·²ç»åœ¨GPUä¸Šï¼Œä¸éœ€è¦æ‰‹åŠ¨ç§»åŠ¨
        if not quantization_config:
            model = model.to(device)
        
        model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        
        # å°è¯•ä½¿ç”¨ torch.compile åŠ é€Ÿï¼ˆPyTorch 2.0+ï¼‰
        try:
            if hasattr(torch, 'compile') and torch.cuda.is_available():
                print("ä½¿ç”¨ torch.compile ä¼˜åŒ–æ¨¡å‹...")
                model = torch.compile(model, mode="reduce-overhead")
        except Exception as e:
            print(f"torch.compile ä¸å¯ç”¨æˆ–å¤±è´¥: {e}")
        
        print("æ¨¡å‹åŠ è½½å®Œæˆï¼")
        
        # æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯
        model_size = sum(p.numel() * p.element_size() for p in model.parameters()) / (1024**2)
        info = f"âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼\n"
        info += f"æ¨¡å‹è·¯å¾„: {model_path}\n"
        info += f"ä½¿ç”¨è®¾å¤‡: {device}\n"
        info += f"é‡åŒ–: {'æ˜¯ (' + quantization_type + ')' if quantization_config else 'å¦'}\n"
        info += f"æ¨¡å‹å¤§å°: {model_size:.1f} MB"
        
        return info
        
    except Exception as e:
        return f"âŒ åŠ è½½æ¨¡å‹æ—¶å‡ºé”™ï¼š{str(e)}\næç¤ºï¼šå¦‚æœä½¿ç”¨é‡åŒ–ï¼Œè¯·ç¡®ä¿å®‰è£…äº† bitsandbytes: pip install bitsandbytes"

def generate_code(prompt, system_prompt, max_tokens, temperature, top_p, use_cache=True):
    """ç”Ÿæˆä»£ç çš„å‡½æ•°ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
    if model is None or tokenizer is None:
        return "é”™è¯¯ï¼šæ¨¡å‹å°šæœªåŠ è½½ï¼Œè¯·å…ˆç‚¹å‡»'åŠ è½½æ¨¡å‹'æŒ‰é’®ã€‚"
    
    if not prompt or prompt.strip() == "":
        return "é”™è¯¯ï¼šè¯·è¾“å…¥ä»£ç ç”Ÿæˆæç¤ºã€‚"
    
    try:
        # å‡†å¤‡å¯¹è¯æ¶ˆæ¯
        messages = [
            {"role": "system", "content": system_prompt if system_prompt else "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¼–ç¨‹åŠ©æ‰‹ï¼Œæ“…é•¿ç¼–å†™å’Œè§£é‡Šä»£ç ã€‚"},
            {"role": "user", "content": prompt},
        ]
        
        # åº”ç”¨èŠå¤©æ¨¡æ¿
        text = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        
        # å°†æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥
        model_inputs = tokenizer([text], return_tensors="pt").to(device)
        
        # ç”Ÿæˆä»£ç ï¼ˆä¼˜åŒ–å‚æ•°ï¼‰
        with torch.no_grad():
            # ä½¿ç”¨ torch.inference_mode() è¿›ä¸€æ­¥ä¼˜åŒ–ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            generated_ids = model.generate(
                **model_inputs,
                max_new_tokens=int(max_tokens),
                temperature=float(temperature),
                top_p=float(top_p),
                do_sample=True,
                use_cache=use_cache,  # ä½¿ç”¨KVç¼“å­˜åŠ é€Ÿ
                pad_token_id=tokenizer.eos_token_id,  # é¿å…è­¦å‘Š
            )
        
        # æå–ç”Ÿæˆçš„æ–‡æœ¬ï¼ˆå»æ‰è¾“å…¥éƒ¨åˆ†ï¼‰
        generated_ids = [
            output_ids[len(input_ids):] 
            for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
        ]
        
        response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
        return response
        
    except Exception as e:
        return f"ç”Ÿæˆä»£ç æ—¶å‡ºé”™ï¼š{str(e)}"

# åˆ›å»º Gradio ç•Œé¢
with gr.Blocks(title="Qwen2.5-Coder æœ¬åœ°æ¨¡å‹ä»£ç ç”Ÿæˆå™¨ï¼ˆä¼˜åŒ–ç‰ˆï¼‰", theme=gr.themes.Soft()) as demo:
    gr.Markdown("# ğŸš€ Qwen2.5-Coder æœ¬åœ°æ¨¡å‹ä»£ç ç”Ÿæˆå™¨ï¼ˆä¼˜åŒ–ç‰ˆï¼‰")
    gr.Markdown("ä½¿ç”¨é‡åŒ–ç­‰æŠ€æœ¯ä¼˜åŒ–çš„æœ¬åœ°æ¨¡å‹ï¼Œæå‡æ¨ç†é€Ÿåº¦ã€‚")
    
    with gr.Row():
        with gr.Column():
            gr.Markdown("### ğŸ“ æ¨¡å‹è®¾ç½®")
            model_path_input = gr.Textbox(
                label="æ¨¡å‹è·¯å¾„",
                value=DEFAULT_MODEL_PATH,
                placeholder="è¾“å…¥æœ¬åœ°æ¨¡å‹è·¯å¾„ï¼Œä¾‹å¦‚: ./Qwen2.5-Coder-1.5B",
                lines=1
            )
            
            with gr.Row():
                use_quantization_check = gr.Checkbox(
                    label="ä½¿ç”¨é‡åŒ–åŠ é€Ÿï¼ˆéœ€è¦GPUï¼‰",
                    value=True,
                    info="ä½¿ç”¨8-bitæˆ–4-bité‡åŒ–å¯ä»¥å¤§å¹…å‡å°‘æ˜¾å­˜å ç”¨å’Œæå‡é€Ÿåº¦"
                )
                quantization_type_dropdown = gr.Dropdown(
                    label="é‡åŒ–ç±»å‹",
                    choices=["8bit", "4bit"],
                    value="8bit",
                    info="4-bitæ›´å¿«ä½†å¯èƒ½ç•¥å¾®é™ä½è´¨é‡"
                )
            
            load_btn = gr.Button("ğŸ”„ åŠ è½½æ¨¡å‹", variant="primary", size="lg")
            load_status = gr.Textbox(label="æ¨¡å‹çŠ¶æ€", interactive=False, lines=5)
            
            with gr.Accordion("âš™ï¸ ç”Ÿæˆå‚æ•°è®¾ç½®", open=False):
                system_prompt_input = gr.Textbox(
                    label="ç³»ç»Ÿæç¤ºè¯",
                    value="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¼–ç¨‹åŠ©æ‰‹ï¼Œæ“…é•¿ç¼–å†™å’Œè§£é‡Šä»£ç ã€‚",
                    lines=2,
                    placeholder="è¾“å…¥ç³»ç»Ÿæç¤ºè¯..."
                )
                max_tokens_input = gr.Slider(
                    label="æœ€å¤§ç”Ÿæˆtokenæ•°",
                    minimum=50,
                    maximum=2048,
                    value=512,
                    step=50
                )
                temperature_input = gr.Slider(
                    label="Temperature (åˆ›é€ æ€§)",
                    minimum=0.1,
                    maximum=2.0,
                    value=0.7,
                    step=0.1
                )
                top_p_input = gr.Slider(
                    label="Top-p (æ ¸é‡‡æ ·)",
                    minimum=0.1,
                    maximum=1.0,
                    value=0.9,
                    step=0.05
                )
                use_cache_check = gr.Checkbox(
                    label="ä½¿ç”¨KVç¼“å­˜åŠ é€Ÿ",
                    value=True,
                    info="å¯ç”¨å¯ä»¥åŠ é€Ÿç”Ÿæˆï¼Œä½†ä¼šå ç”¨æ›´å¤šæ˜¾å­˜"
                )
        
        with gr.Column():
            gr.Markdown("### ğŸ’» ä»£ç ç”Ÿæˆ")
            prompt_input = gr.Textbox(
                label="ä»£ç ç”Ÿæˆæç¤º",
                placeholder="ä¾‹å¦‚ï¼šè¯·ç”¨Pythonç¼–å†™ä¸€ä¸ªå¿«é€Ÿæ’åºç®—æ³•ã€‚",
                lines=5
            )
            generate_btn = gr.Button("âœ¨ ç”Ÿæˆä»£ç ", variant="primary", size="lg")
            output = gr.Code(
                label="ç”Ÿæˆçš„ä»£ç ",
                language="python",
                lines=20
            )
    
    # ç»‘å®šäº‹ä»¶
    def load_model_wrapper(model_path, use_quant, quant_type):
        return load_model(model_path, use_quant, quant_type)
    
    load_btn.click(
        fn=load_model_wrapper,
        inputs=[model_path_input, use_quantization_check, quantization_type_dropdown],
        outputs=load_status
    )
    
    generate_btn.click(
        fn=generate_code,
        inputs=[prompt_input, system_prompt_input, max_tokens_input, temperature_input, top_p_input, use_cache_check],
        outputs=output
    )
    
    # ç¤ºä¾‹æç¤ºè¯
    gr.Examples(
        examples=[
            ["è¯·ç”¨Pythonç¼–å†™ä¸€ä¸ªå¿«é€Ÿæ’åºç®—æ³•ã€‚"],
            ["ç”¨Pythonå®ç°ä¸€ä¸ªç®€å•çš„HTTPæœåŠ¡å™¨ã€‚"],
            ["å†™ä¸€ä¸ªå‡½æ•°æ¥è®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—çš„ç¬¬né¡¹ã€‚"],
            ["ç”¨Pythonå®ç°ä¸€ä¸ªç®€å•çš„è®¡ç®—å™¨ç±»ã€‚"],
        ],
        inputs=prompt_input
    )
    
    # æ·»åŠ è¯´æ˜
    gr.Markdown("""
    ### ğŸ’¡ ä¼˜åŒ–è¯´æ˜
    
    **ä¸ºä»€ä¹ˆæœ¬åœ°æ¨¡å‹æ¯”APIæ…¢ï¼Ÿ**
    - APIæœåŠ¡å™¨é€šå¸¸ä½¿ç”¨å¼ºå¤§çš„GPUé›†ç¾¤
    - æœ¬åœ°å¯èƒ½ä½¿ç”¨CPUè¿è¡Œï¼ˆCPUæ¯”GPUæ…¢å¾ˆå¤šï¼‰
    - æ²¡æœ‰ä½¿ç”¨é‡åŒ–ç­‰ä¼˜åŒ–æŠ€æœ¯
    
    **æœ¬ä¼˜åŒ–ç‰ˆæœ¬åŒ…å«ï¼š**
    - âœ… 8-bit/4-bité‡åŒ–ï¼šå‡å°‘æ˜¾å­˜å ç”¨ï¼Œæå‡é€Ÿåº¦ï¼ˆéœ€è¦GPUï¼‰
    - âœ… KVç¼“å­˜ï¼šåŠ é€Ÿç”Ÿæˆè¿‡ç¨‹
    - âœ… torch.compileï¼šPyTorch 2.0+ç¼–è¯‘ä¼˜åŒ–
    - âœ… åŠç²¾åº¦æ¨ç†ï¼šå‡å°‘æ˜¾å­˜å ç”¨
    
    **å¦‚æœä»ç„¶å¾ˆæ…¢ï¼š**
    - ç¡®ä¿ä½¿ç”¨GPUï¼ˆé‡åŒ–éœ€è¦GPUï¼‰
    - å®‰è£… bitsandbytes: `pip install bitsandbytes`
    - è€ƒè™‘ä½¿ç”¨æ›´å°çš„æ¨¡å‹æˆ–æ›´ä¸“ä¸šçš„æ¨ç†åº“ï¼ˆå¦‚vLLMï¼‰
    """)

if __name__ == "__main__":
    # å¯åŠ¨ Gradio ç•Œé¢
    demo.launch(share=False, server_name="0.0.0.0", server_port=7860)

